{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuE_RWUy3GgG",
        "outputId": "0170fc09-26d7-4977-df5c-da51fdeb14a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading dataset (use GoogleDrive)"
      ],
      "metadata": {
        "id": "pcScPstxQ193"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqhlNIST4Y06",
        "outputId": "064da700-495b-430f-ab8d-44e14af5f978"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "train_data_path = '/content/drive/MyDrive/Text_Mining/Datasets/train_wiki.json'\n",
        "test_data_path = '/content/drive/MyDrive/Text_Mining/Datasets/val_wiki.json'\n",
        "wiki_pid_path = '/content/drive/MyDrive/Text_Mining/Datasets/pid2name.json'\n",
        "\n",
        "with open(train_data_path, 'r') as file:\n",
        "    train_data = json.load(file)\n",
        "    print(train_data)\n",
        "with open(test_data_path, 'r') as file:\n",
        "    val_data = json.load(file)\n",
        "    print(val_data)\n",
        "with open(wiki_pid_path, 'r') as file:\n",
        "    wiki_pid = json.load(file)\n",
        "    print(wiki_pid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpqbHcr48D0G",
        "outputId": "23cef397-dd78-430e-87ca-2142abf44ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: It was first described by German botanist Conrad Moench .\n",
            "Head Entity: ['conrad moench', 'Q60948', [[7, 8]]]\n",
            "Tail Entity: ['botanist', 'Q441', [[6]]]\n",
            "Relation: P101 = location of formation\n"
          ]
        }
      ],
      "source": [
        "sample = train_data['P101'][0]\n",
        "print(\"Sentence:\", \" \".join(sample[\"tokens\"]))\n",
        "print(\"Head Entity:\", sample[\"h\"])\n",
        "print(\"Tail Entity:\", sample[\"t\"])\n",
        "print(\"Relation:\", \"P101 =\",wiki_pid['P740'][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD_QFDIYUwYu",
        "outputId": "d612074d-5d78-49a4-e298-bd6f414a611f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tokens': ['It', 'was', 'first', 'described', 'by', 'German', 'botanist', 'Conrad', 'Moench', '.'], 'h': ['conrad moench', 'Q60948', [[7, 8]]], 't': ['botanist', 'Q441', [[6]]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "Iri99p51Q6rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Loading dataset\n",
        "def load_fewrel_data1(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    for relation, samples in data.items():\n",
        "      for sample in samples:\n",
        "          sentence = \" \".join(sample[\"tokens\"])\n",
        "          sentences.append(sentence)\n",
        "          labels.append(relation)\n",
        "    return sentences, labels\n",
        "\n",
        "# Split the dataset\n",
        "sentences,labels = load_fewrel_data1(train_data_path)\n",
        "train_sentences,test_sentences,train_labels,test_labels = train_test_split(sentences,labels,test_size=0.2,stratify=labels,random_state=42)\n",
        "\n",
        "# Calculate Tf-Idf\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_sentences)\n",
        "X_test = vectorizer.transform(test_sentences)\n",
        "\n",
        "# SVM training\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "svm_model.fit(X_train, train_labels)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(test_labels, y_pred))\n",
        "\n",
        "# Inference\n",
        "def infer(sentence):\n",
        "    sentence_vector = vectorizer.transform([sentence])\n",
        "    predicted_relation = svm_model.predict(sentence_vector)\n",
        "    return predicted_relation[0]\n",
        "\n",
        "# Testing\n",
        "test_sentence = \"Google was founded in California.\"\n",
        "predicted_relation = infer(test_sentence)\n",
        "print(f\"Predicted Relation: {predicted_relation}, {predicted_relation} = {wiki_pid[predicted_relation]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN0jSjXYKlnj",
        "outputId": "95a37abe-7c7c-4bbc-817a-be610767ce2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       P1001       0.47      0.52      0.49       140\n",
            "        P101       0.46      0.48      0.47       140\n",
            "        P102       0.59      0.69      0.64       140\n",
            "        P105       0.89      0.97      0.93       140\n",
            "        P106       0.52      0.54      0.53       140\n",
            "        P118       0.60      0.73      0.66       140\n",
            "        P123       0.44      0.51      0.47       140\n",
            "        P127       0.32      0.32      0.32       140\n",
            "       P1303       0.76      0.71      0.74       140\n",
            "        P131       0.32      0.23      0.27       140\n",
            "       P1344       0.67      0.59      0.63       140\n",
            "       P1346       0.54      0.69      0.60       140\n",
            "        P135       0.57      0.64      0.60       140\n",
            "        P136       0.45      0.38      0.41       140\n",
            "        P137       0.58      0.62      0.60       140\n",
            "        P140       0.73      0.70      0.71       140\n",
            "       P1408       0.86      0.81      0.84       140\n",
            "       P1411       0.85      0.94      0.89       140\n",
            "       P1435       0.94      1.00      0.97       140\n",
            "        P150       0.48      0.45      0.46       140\n",
            "        P156       0.29      0.28      0.28       140\n",
            "        P159       0.30      0.28      0.29       140\n",
            "         P17       0.31      0.23      0.26       140\n",
            "        P175       0.43      0.46      0.44       140\n",
            "        P176       0.58      0.57      0.58       140\n",
            "        P178       0.46      0.49      0.47       140\n",
            "       P1877       0.54      0.64      0.58       140\n",
            "       P1923       0.65      0.66      0.66       140\n",
            "         P22       0.58      0.71      0.64       140\n",
            "        P241       0.80      0.89      0.84       140\n",
            "        P264       0.71      0.79      0.74       140\n",
            "         P27       0.28      0.21      0.24       140\n",
            "        P276       0.30      0.34      0.32       140\n",
            "        P306       0.79      0.89      0.84       140\n",
            "         P31       0.26      0.16      0.20       140\n",
            "       P3373       0.65      0.66      0.65       140\n",
            "       P3450       0.55      0.60      0.57       140\n",
            "        P355       0.50      0.46      0.48       140\n",
            "         P39       0.36      0.36      0.36       140\n",
            "        P400       0.69      0.72      0.71       140\n",
            "        P403       0.47      0.37      0.42       140\n",
            "        P407       0.64      0.53      0.58       140\n",
            "        P449       0.68      0.77      0.72       140\n",
            "       P4552       0.83      0.89      0.86       140\n",
            "        P460       0.59      0.61      0.60       140\n",
            "        P466       0.64      0.70      0.67       140\n",
            "        P495       0.49      0.34      0.40       140\n",
            "        P527       0.25      0.23      0.24       140\n",
            "        P551       0.30      0.25      0.27       140\n",
            "         P57       0.52      0.59      0.55       140\n",
            "         P58       0.61      0.49      0.54       140\n",
            "          P6       0.55      0.56      0.56       140\n",
            "        P674       0.50      0.50      0.50       140\n",
            "        P706       0.59      0.63      0.61       140\n",
            "        P710       0.47      0.49      0.48       140\n",
            "        P740       0.50      0.41      0.45       140\n",
            "        P750       0.73      0.64      0.68       140\n",
            "        P800       0.31      0.26      0.28       140\n",
            "         P84       0.74      0.82      0.78       140\n",
            "         P86       0.57      0.55      0.56       140\n",
            "        P931       0.86      0.86      0.86       140\n",
            "        P937       0.38      0.29      0.33       140\n",
            "        P974       0.52      0.63      0.57       140\n",
            "        P991       0.67      0.78      0.72       140\n",
            "\n",
            "    accuracy                           0.56      8960\n",
            "   macro avg       0.55      0.56      0.56      8960\n",
            "weighted avg       0.55      0.56      0.56      8960\n",
            "\n",
            "Predicted Relation: P740, P740 = ['location of formation', 'location where a group or organization was formed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancement\n",
        "\n",
        "In this section, I perform entity marking to force the model to focus on the entity context. In this way, the model does not need to infer entity locations on its own and can focus directly on the semantic links between entities."
      ],
      "metadata": {
        "id": "HhIq0NBER6Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "m-m4BJHqSmO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def entity_marking(sample):\n",
        "    tokens = sample[\"tokens\"]\n",
        "    '''\n",
        "    This function is to mark the entities in a sentence.\n",
        "    {'P931': [{'tokens': ['Merpati', 'flight', '106', 'departed', 'Jakarta', '(', 'CGK', ')', 'on', 'a', 'domestic', 'flight', 'to', 'Tanjung', 'Pandan', '(', 'TJQ', ')', '.'],\n",
        "    'h': ['tjq', 'Q1331049', [[16]]], 't': ['tanjung pandan', 'Q3056359', [[13, 14]]]}\n",
        "    '''\n",
        "\n",
        "    h_pos = sample[\"h\"][2][0]  # Get head position\n",
        "    h_start = h_pos[0]\n",
        "    h_end = h_pos[-1]  # If the entity is a single token, h_end = h_start\n",
        "    # Get tail entity position\n",
        "    t_pos = sample[\"t\"][2][0]\n",
        "    t_start = t_pos[0]\n",
        "    t_end = t_pos[-1]\n",
        "    # Add entity markers\n",
        "    tokens[h_start] = \"[E1]\" + tokens[h_start]\n",
        "    tokens[h_end] += \"[/E1]\"\n",
        "    tokens[t_start] = \"[E2]\" + tokens[t_start]\n",
        "    tokens[t_end] += \"[/E2]\"\n",
        "    sentence = \" \".join(tokens)\n",
        "    return sentence\n",
        "\n",
        "def load_fewrel_data2(file_path):\n",
        "    # load the dataset and perform entity marking\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    sentences,labels = [],[]\n",
        "    for relation,samples in data.items():\n",
        "      for sample in samples:\n",
        "        sentence = entity_marking(sample)\n",
        "        sentences.append(sentence)\n",
        "        labels.append(relation)\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "def enrich_text(text):\n",
        "    # Extract linguistic features\n",
        "    doc = nlp(text)\n",
        "    features = []\n",
        "    for token in doc:\n",
        "        features.append(f\"{token.text}_{token.pos_}_{token.dep_}\")\n",
        "    return \" \".join(features)\n",
        "\n",
        "def ProcessSentenceForPre(text):\n",
        "    \"\"\"\n",
        "    This function is to convert a normal sentence into a FewRel structure.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    entities = [(ent.text, ent.start, ent.end, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    if len(entities) < 2:\n",
        "      return \"Not enough entities detected for relation extraction.\"\n",
        "    head, tail = entities[0], entities[1]\n",
        "    h_text, h_start, h_end, h_type = head\n",
        "    t_text, t_start, t_end, t_type = tail\n",
        "\n",
        "    tokens[h_start] = \"[E1]\" + tokens[h_start]\n",
        "    tokens[h_end - 1] += \"[/E1]\"\n",
        "    tokens[t_start] = \"[E2]\" + tokens[t_start]\n",
        "    tokens[t_end - 1] += \"[/E2]\"\n",
        "    processed_sentence = {\n",
        "      \"tokens\": tokens,\n",
        "      \"h\": [h_text, h_type, [[h_start, h_end - 1]]],  # Head entity\n",
        "      \"t\": [t_text, t_type, [[t_start, t_end - 1]]]   # Tail entity\n",
        "    }\n",
        "\n",
        "    return processed_sentence\n",
        "\n",
        "# Example\n",
        "test_sentence = \"Google was founded in California.\"\n",
        "fewrel_format = ProcessSentenceForPre(test_sentence)\n",
        "print(\"FewRel-Formatted Output:\\n\", fewrel_format)"
      ],
      "metadata": {
        "id": "N1QCTcIqR55M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e77132-c7d4-4671-9e67-776302b7376f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FewRel-Formatted Output:\n",
            " {'tokens': ['[E1]Google[/E1]', 'was', 'founded', 'in', '[E2]California[/E2]', '.'], 'h': ['Google', 'ORG', [[0, 0]]], 't': ['California', 'GPE', [[4, 4]]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Merpati flight 106 departed Jakarta ( CGK ) on a domestic flight to [E2]Tanjung Pandan[/E2] ( [E1]TJQ[/E1] ) .\"\n",
        "print(enrich_text(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4m7kEI0VcPy",
        "outputId": "21625c7a-c112-46b9-c7c5-37d0b28f31e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merpati_PROPN_compound flight_NOUN_nsubj 106_NUM_nummod departed_VERB_ROOT Jakarta_PROPN_npadvmod (_PUNCT_punct CGK_PROPN_appos )_PUNCT_punct on_ADP_prep a_DET_det domestic_ADJ_amod flight_NOUN_pobj to_ADP_prep [_PUNCT_dep E2]Tanjung_PROPN_compound Pandan[/E2_PROPN_pobj ]_PUNCT_punct (_PUNCT_punct [_X_nmod E1]TJQ[/E1_NOUN_ROOT ]_PUNCT_punct )_PUNCT_punct ._PUNCT_punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "sentences,labels = load_fewrel_data2(train_data_path)\n",
        "# print(sentences)\n",
        "# sentences = [enrich_text(s) for s in sentences]\n",
        "train_sentences,test_sentences,train_labels,test_labels = train_test_split(sentences,labels,test_size=0.2,stratify=labels,random_state=42)"
      ],
      "metadata": {
        "id": "BYkZLq7dSAQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,\n",
        "    ngram_range=(1, 3),  # Capture word combinations of size 1-3\n",
        "    stop_words='english',\n",
        "    sublinear_tf=True    # Use 1+log(tf) instead of raw term frequency\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_sentences)\n",
        "X_test = vectorizer.transform(test_sentences)\n",
        "\n",
        "# SVM training\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "svm_model.fit(X_train, train_labels)\n",
        "\n",
        "# param_grid = {\n",
        "#     'C': [0.1, 1, 10, 100],\n",
        "#     'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "#     'kernel': ['rbf', 'linear']\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='f1_macro')\n",
        "# grid_search.fit(X_train, train_labels)\n",
        "# print(\"Best parameters:\", grid_search.best_params_)\n",
        "# svm_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "eZT4IzQZSpkS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "b83d9403-5417-4541-e06f-2c12229a64b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation, print the classification performance on training set and testing set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(\"Training accuracy:\\n\",classification_report(train_labels,svm_model.predict(X_train)))\n",
        "print(\"Classification Report:\\n\", classification_report(test_labels, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oxl6_SEyQs-",
        "outputId": "216f92a9-6afa-456e-a22c-44c2779ba26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       P1001       0.90      0.90      0.90       560\n",
            "        P101       0.90      0.93      0.91       560\n",
            "        P102       0.95      0.96      0.95       560\n",
            "        P105       1.00      0.99      0.99       560\n",
            "        P106       0.96      0.93      0.95       560\n",
            "        P118       0.97      0.98      0.97       560\n",
            "        P123       0.90      0.91      0.91       560\n",
            "        P127       0.85      0.80      0.83       560\n",
            "       P1303       0.99      0.97      0.98       560\n",
            "        P131       0.89      0.80      0.84       560\n",
            "       P1344       0.95      0.98      0.96       560\n",
            "       P1346       0.90      0.94      0.92       560\n",
            "        P135       0.94      0.95      0.95       560\n",
            "        P136       0.98      0.90      0.94       560\n",
            "        P137       0.90      0.91      0.90       560\n",
            "        P140       0.96      0.95      0.96       560\n",
            "       P1408       0.95      0.98      0.96       560\n",
            "       P1411       0.98      1.00      0.99       560\n",
            "       P1435       1.00      1.00      1.00       560\n",
            "        P150       0.89      0.92      0.91       560\n",
            "        P156       0.88      0.84      0.86       560\n",
            "        P159       0.81      0.86      0.83       560\n",
            "         P17       0.92      0.80      0.86       560\n",
            "        P175       0.86      0.93      0.90       560\n",
            "        P176       0.90      0.94      0.92       560\n",
            "        P178       0.86      0.94      0.90       560\n",
            "       P1877       0.84      0.93      0.89       560\n",
            "       P1923       0.91      0.97      0.94       560\n",
            "         P22       0.89      0.96      0.92       560\n",
            "        P241       0.95      0.98      0.96       560\n",
            "        P264       0.95      0.96      0.96       560\n",
            "         P27       0.93      0.90      0.91       560\n",
            "        P276       0.89      0.83      0.86       560\n",
            "        P306       0.93      0.97      0.95       560\n",
            "         P31       0.92      0.75      0.83       560\n",
            "       P3373       0.93      0.93      0.93       560\n",
            "       P3450       0.94      0.97      0.96       560\n",
            "        P355       0.90      0.90      0.90       560\n",
            "         P39       0.96      0.95      0.96       560\n",
            "        P400       0.95      0.94      0.94       560\n",
            "        P403       0.92      0.90      0.91       560\n",
            "        P407       0.94      0.95      0.94       560\n",
            "        P449       0.94      0.98      0.96       560\n",
            "       P4552       0.91      0.99      0.95       560\n",
            "        P460       0.89      0.90      0.90       560\n",
            "        P466       0.94      0.95      0.94       560\n",
            "        P495       0.90      0.92      0.91       560\n",
            "        P527       0.92      0.77      0.84       560\n",
            "        P551       0.90      0.81      0.85       560\n",
            "         P57       0.86      0.94      0.90       560\n",
            "         P58       0.93      0.78      0.85       560\n",
            "          P6       0.89      0.97      0.93       560\n",
            "        P674       0.89      0.94      0.92       560\n",
            "        P706       0.90      0.90      0.90       560\n",
            "        P710       0.90      0.91      0.90       560\n",
            "        P740       0.91      0.85      0.88       560\n",
            "        P750       0.95      0.91      0.93       560\n",
            "        P800       0.92      0.85      0.88       560\n",
            "         P84       0.93      0.99      0.96       560\n",
            "         P86       0.92      0.92      0.92       560\n",
            "        P931       0.94      0.95      0.95       560\n",
            "        P937       0.89      0.86      0.87       560\n",
            "        P974       0.89      0.93      0.91       560\n",
            "        P991       0.94      0.97      0.96       560\n",
            "\n",
            "    accuracy                           0.92     35840\n",
            "   macro avg       0.92      0.92      0.92     35840\n",
            "weighted avg       0.92      0.92      0.92     35840\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       P1001       0.70      0.67      0.69       140\n",
            "        P101       0.52      0.60      0.56       140\n",
            "        P102       0.78      0.80      0.79       140\n",
            "        P105       0.98      0.95      0.96       140\n",
            "        P106       0.78      0.71      0.74       140\n",
            "        P118       0.74      0.81      0.77       140\n",
            "        P123       0.62      0.61      0.61       140\n",
            "        P127       0.36      0.37      0.37       140\n",
            "       P1303       0.94      0.81      0.87       140\n",
            "        P131       0.58      0.47      0.52       140\n",
            "       P1344       0.84      0.84      0.84       140\n",
            "       P1346       0.63      0.79      0.70       140\n",
            "        P135       0.69      0.78      0.73       140\n",
            "        P136       0.85      0.66      0.75       140\n",
            "        P137       0.62      0.65      0.63       140\n",
            "        P140       0.81      0.79      0.80       140\n",
            "       P1408       0.88      0.87      0.88       140\n",
            "       P1411       0.91      0.96      0.93       140\n",
            "       P1435       0.98      0.99      0.99       140\n",
            "        P150       0.70      0.67      0.69       140\n",
            "        P156       0.34      0.37      0.35       140\n",
            "        P159       0.41      0.45      0.43       140\n",
            "         P17       0.61      0.54      0.57       140\n",
            "        P175       0.60      0.69      0.64       140\n",
            "        P176       0.63      0.66      0.65       140\n",
            "        P178       0.54      0.66      0.59       140\n",
            "       P1877       0.64      0.69      0.66       140\n",
            "       P1923       0.80      0.76      0.78       140\n",
            "         P22       0.70      0.79      0.74       140\n",
            "        P241       0.89      0.91      0.90       140\n",
            "        P264       0.84      0.86      0.85       140\n",
            "         P27       0.56      0.50      0.53       140\n",
            "        P276       0.42      0.42      0.42       140\n",
            "        P306       0.83      0.93      0.88       140\n",
            "         P31       0.45      0.35      0.39       140\n",
            "       P3373       0.73      0.71      0.72       140\n",
            "       P3450       0.72      0.76      0.74       140\n",
            "        P355       0.59      0.61      0.60       140\n",
            "         P39       0.77      0.66      0.71       140\n",
            "        P400       0.81      0.75      0.78       140\n",
            "        P403       0.74      0.68      0.71       140\n",
            "        P407       0.82      0.71      0.76       140\n",
            "        P449       0.80      0.79      0.79       140\n",
            "       P4552       0.85      0.91      0.88       140\n",
            "        P460       0.60      0.61      0.61       140\n",
            "        P466       0.74      0.74      0.74       140\n",
            "        P495       0.65      0.59      0.62       140\n",
            "        P527       0.29      0.31      0.30       140\n",
            "        P551       0.40      0.33      0.36       140\n",
            "         P57       0.65      0.74      0.69       140\n",
            "         P58       0.67      0.51      0.58       140\n",
            "          P6       0.71      0.86      0.77       140\n",
            "        P674       0.55      0.61      0.58       140\n",
            "        P706       0.64      0.62      0.63       140\n",
            "        P710       0.72      0.63      0.67       140\n",
            "        P740       0.66      0.56      0.61       140\n",
            "        P750       0.84      0.76      0.80       140\n",
            "        P800       0.40      0.42      0.41       140\n",
            "         P84       0.79      0.83      0.81       140\n",
            "         P86       0.62      0.71      0.66       140\n",
            "        P931       0.90      0.88      0.89       140\n",
            "        P937       0.56      0.49      0.52       140\n",
            "        P974       0.73      0.76      0.75       140\n",
            "        P991       0.85      0.85      0.85       140\n",
            "\n",
            "    accuracy                           0.68      8960\n",
            "   macro avg       0.69      0.68      0.68      8960\n",
            "weighted avg       0.69      0.68      0.68      8960\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Save the model\n",
        "joblib.dump(svm_model, 'svm_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUzp04zL2ipO",
        "outputId": "9d8e92cf-687f-4f09-f6ec-0a917d665670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_svm_model = joblib.load('svm_model.pkl')\n",
        "loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "id": "v0d-DZXN6eOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "def infer(sentence,model,vectorizer):\n",
        "    sentence = ProcessSentenceForPre(sentence)\n",
        "    sentence = entity_marking(sentence)\n",
        "    sentence_vector = vectorizer.transform([sentence])\n",
        "    predicted_relation = model.predict(sentence_vector)\n",
        "    return predicted_relation[0]"
      ],
      "metadata": {
        "id": "UGtUGIl4SupN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_svm_model = joblib.load('svm_model.pkl')\n",
        "loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "test_sentence = \"Barack Obama was born in Honolulu, Hawaii.\"\n",
        "predicted_relation = infer(test_sentence,loaded_svm_model,loaded_vectorizer)\n",
        "print(f\"Predicted Relation: {predicted_relation}, {predicted_relation} = {wiki_pid[predicted_relation]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7kD-S0G6keA",
        "outputId": "22057ef0-059e-4b2a-aaf4-b86615c9fa2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Relation: P551, P551 = ['residence', 'the place where the person is or has been, resident']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM implemented by deep learning\n",
        "In this section, I tried a different way to implement `SVC` model using deep learning language `Pytorch`. The advantages of using PyTorch to implement Support Vector Classifiers lie in the combination of flexibility and the depth of the deep learning method. This idea was inspired by ChatGPT and DeepSeek.\n",
        "\n",
        "Code reference:\n",
        "\n",
        "https://github.com/kazuto1011/svm-pytorch\n",
        "\n",
        "https://github.com/USHAHANE/T/blob/185bd7e5bf02b2b44b7e4e2fb35349eab4bd7d20/SVM.txt#L39"
      ],
      "metadata": {
        "id": "Qa0NGTaiJ-mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics.pairwise import rbf_kernel"
      ],
      "metadata": {
        "id": "XkstLADGKP6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HA_VjI2Kt0j",
        "outputId": "9d8183cf-c04a-486d-e00f-ab4528af82ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "sentences,labels = load_fewrel_data2(train_data_path)\n",
        "# sentences = [enrich_text(s) for s in sentences]\n",
        "train_sentences,test_sentences,train_labels,test_labels = train_test_split(sentences,labels,test_size=0.2,stratify=labels,random_state=42)"
      ],
      "metadata": {
        "id": "frQnYZUTiRWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 3),  # Capture word combinations of size 1-3\n",
        "    stop_words='english',\n",
        "    sublinear_tf=True    # Use 1+log(tf) instead of raw term frequency\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_sentences).toarray().astype(np.float32)\n",
        "X_test = vectorizer.transform(test_sentences).toarray().astype(np.float32)"
      ],
      "metadata": {
        "id": "IYoXKZgPKaOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "label_to_idx = {label: idx for idx, label in enumerate(set(labels))}\n",
        "y_train = torch.tensor([label_to_idx[label] for label in train_labels], dtype=torch.long)\n",
        "y_test = torch.tensor([label_to_idx[label] for label in test_labels], dtype=torch.long)"
      ],
      "metadata": {
        "id": "pESagPmYKhwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), y_train)\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "CJI_zWrSY3IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture of SVC\n",
        "class SVCDL(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JIiY2ARdO2lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_labels = len(set(labels))\n",
        "model = SVCDL(X_train.shape[1],num_labels)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005,momentum=0.9, weight_decay=0.001)\n",
        "criterion = nn.MultiMarginLoss(margin=1.0,p=1)\n",
        "model.train()\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for batch_x,batch_y in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(batch_x)\n",
        "      loss = criterion(outputs, batch_y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "      print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "jlekC3PeVVNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bad4b37-273c-42fd-ea05-348f20575d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.6977\n",
            "Epoch 20, Loss: 0.5853\n",
            "Epoch 30, Loss: 0.5429\n",
            "Epoch 40, Loss: 0.5253\n",
            "Epoch 50, Loss: 0.5171\n",
            "Epoch 60, Loss: 0.5131\n",
            "Epoch 70, Loss: 0.5111\n",
            "Epoch 80, Loss: 0.5099\n",
            "Epoch 90, Loss: 0.5093\n",
            "Epoch 100, Loss: 0.5090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the classification performance on training set and testing set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "        outputs = model(X)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "    print(\"Classification report on training set\\n\")\n",
        "    print(classification_report(all_labels, all_preds))\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for X, y in test_loader:\n",
        "        outputs = model(X)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "    print(\"Classification report on testing set\\n\")\n",
        "    print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiDklbmDVzIT",
        "outputId": "1ed101cc-0b35-4b8c-ebf5-8a547683c3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report on training set\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.29      0.36       560\n",
            "           1       0.70      0.73      0.72       560\n",
            "           2       0.80      0.61      0.69       560\n",
            "           3       0.76      0.70      0.72       560\n",
            "           4       0.87      0.71      0.78       560\n",
            "           5       1.00      0.84      0.91       560\n",
            "           6       0.93      0.60      0.73       560\n",
            "           7       0.52      0.70      0.60       560\n",
            "           8       0.59      0.52      0.55       560\n",
            "           9       0.65      0.70      0.67       560\n",
            "          10       0.42      0.41      0.41       560\n",
            "          11       0.47      0.65      0.54       560\n",
            "          12       0.81      0.49      0.61       560\n",
            "          13       0.68      0.52      0.59       560\n",
            "          14       0.65      0.65      0.65       560\n",
            "          15       0.80      0.51      0.62       560\n",
            "          16       0.68      0.75      0.71       560\n",
            "          17       0.84      0.84      0.84       560\n",
            "          18       0.65      0.70      0.67       560\n",
            "          19       0.66      0.43      0.52       560\n",
            "          20       0.48      0.56      0.52       560\n",
            "          21       0.49      0.59      0.54       560\n",
            "          22       0.79      0.72      0.76       560\n",
            "          23       0.92      0.87      0.89       560\n",
            "          24       0.30      0.63      0.41       560\n",
            "          25       0.78      0.55      0.64       560\n",
            "          26       0.58      0.63      0.60       560\n",
            "          27       0.30      0.49      0.37       560\n",
            "          28       0.85      0.76      0.81       560\n",
            "          29       0.32      0.42      0.36       560\n",
            "          30       0.46      0.31      0.37       560\n",
            "          31       0.40      0.41      0.40       560\n",
            "          32       0.85      0.83      0.84       560\n",
            "          33       0.65      0.57      0.61       560\n",
            "          34       0.29      0.52      0.37       560\n",
            "          35       0.17      0.33      0.22       560\n",
            "          36       0.66      0.46      0.54       560\n",
            "          37       0.62      0.63      0.62       560\n",
            "          38       0.73      0.46      0.57       560\n",
            "          39       0.91      0.64      0.75       560\n",
            "          40       0.98      0.83      0.90       560\n",
            "          41       0.74      0.67      0.70       560\n",
            "          42       0.60      0.45      0.52       560\n",
            "          43       0.57      0.45      0.50       560\n",
            "          44       0.56      0.46      0.51       560\n",
            "          45       0.61      0.53      0.57       560\n",
            "          46       0.78      0.57      0.66       560\n",
            "          47       0.18      0.53      0.27       560\n",
            "          48       0.86      0.59      0.70       560\n",
            "          49       0.21      0.51      0.30       560\n",
            "          50       0.35      0.33      0.34       560\n",
            "          51       0.91      0.48      0.63       560\n",
            "          52       0.91      0.74      0.82       560\n",
            "          53       0.80      0.71      0.75       560\n",
            "          54       0.89      0.85      0.87       560\n",
            "          55       0.42      0.59      0.49       560\n",
            "          56       0.69      0.66      0.68       560\n",
            "          57       0.77      0.62      0.69       560\n",
            "          58       0.85      0.44      0.58       560\n",
            "          59       0.48      0.28      0.36       560\n",
            "          60       0.61      0.58      0.60       560\n",
            "          61       0.91      0.55      0.69       560\n",
            "          62       0.51      0.58      0.54       560\n",
            "          63       0.83      0.56      0.67       560\n",
            "\n",
            "    accuracy                           0.58     35840\n",
            "   macro avg       0.65      0.58      0.60     35840\n",
            "weighted avg       0.65      0.58      0.60     35840\n",
            "\n",
            "Classification report on testing set\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.25      0.30       140\n",
            "           1       0.68      0.66      0.67       140\n",
            "           2       0.72      0.60      0.65       140\n",
            "           3       0.76      0.72      0.74       140\n",
            "           4       0.83      0.69      0.75       140\n",
            "           5       1.00      0.83      0.91       140\n",
            "           6       0.88      0.59      0.70       140\n",
            "           7       0.50      0.76      0.60       140\n",
            "           8       0.57      0.51      0.54       140\n",
            "           9       0.56      0.54      0.55       140\n",
            "          10       0.36      0.32      0.34       140\n",
            "          11       0.37      0.49      0.42       140\n",
            "          12       0.81      0.41      0.54       140\n",
            "          13       0.60      0.50      0.54       140\n",
            "          14       0.61      0.61      0.61       140\n",
            "          15       0.72      0.47      0.57       140\n",
            "          16       0.65      0.70      0.67       140\n",
            "          17       0.79      0.81      0.80       140\n",
            "          18       0.63      0.65      0.64       140\n",
            "          19       0.62      0.39      0.48       140\n",
            "          20       0.40      0.51      0.45       140\n",
            "          21       0.48      0.59      0.53       140\n",
            "          22       0.76      0.69      0.72       140\n",
            "          23       0.91      0.93      0.92       140\n",
            "          24       0.26      0.54      0.35       140\n",
            "          25       0.75      0.51      0.60       140\n",
            "          26       0.52      0.52      0.52       140\n",
            "          27       0.24      0.44      0.31       140\n",
            "          28       0.88      0.78      0.83       140\n",
            "          29       0.26      0.30      0.28       140\n",
            "          30       0.28      0.21      0.24       140\n",
            "          31       0.32      0.36      0.34       140\n",
            "          32       0.87      0.81      0.84       140\n",
            "          33       0.65      0.58      0.61       140\n",
            "          34       0.20      0.36      0.26       140\n",
            "          35       0.11      0.23      0.15       140\n",
            "          36       0.57      0.40      0.47       140\n",
            "          37       0.55      0.59      0.57       140\n",
            "          38       0.68      0.41      0.51       140\n",
            "          39       0.90      0.65      0.76       140\n",
            "          40       0.98      0.87      0.92       140\n",
            "          41       0.67      0.63      0.65       140\n",
            "          42       0.52      0.37      0.43       140\n",
            "          43       0.55      0.41      0.47       140\n",
            "          44       0.56      0.46      0.50       140\n",
            "          45       0.47      0.41      0.44       140\n",
            "          46       0.78      0.50      0.61       140\n",
            "          47       0.14      0.44      0.21       140\n",
            "          48       0.85      0.59      0.69       140\n",
            "          49       0.18      0.44      0.25       140\n",
            "          50       0.23      0.23      0.23       140\n",
            "          51       0.89      0.41      0.56       140\n",
            "          52       0.90      0.68      0.78       140\n",
            "          53       0.76      0.65      0.70       140\n",
            "          54       0.91      0.84      0.87       140\n",
            "          55       0.39      0.52      0.45       140\n",
            "          56       0.69      0.65      0.67       140\n",
            "          57       0.77      0.56      0.65       140\n",
            "          58       0.75      0.35      0.48       140\n",
            "          59       0.48      0.29      0.36       140\n",
            "          60       0.55      0.51      0.53       140\n",
            "          61       0.88      0.60      0.71       140\n",
            "          62       0.45      0.47      0.46       140\n",
            "          63       0.79      0.50      0.61       140\n",
            "\n",
            "    accuracy                           0.54      8960\n",
            "   macro avg       0.61      0.54      0.56      8960\n",
            "weighted avg       0.61      0.54      0.56      8960\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "def infer(sentence):\n",
        "    '''\n",
        "    This function is to infer the relation between the entity in a sentence.\n",
        "    '''\n",
        "    sentence = ProcessSentenceForPre(sentence)\n",
        "    sentence = entity_marking(sentence)\n",
        "    sentence_vector = vectorizer.transform([sentence]).toarray().astype(np.float32)\n",
        "    sentence_tensor = torch.tensor(sentence_vector, dtype=torch.float32)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(sentence_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        confidence, predicted_idx = torch.max(probabilities, dim=1)\n",
        "        confidence = confidence.item()\n",
        "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
        "    return idx_to_label[predicted_idx.item()]"
      ],
      "metadata": {
        "id": "DVYzcDjeW28N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test_sentence = \"Google was founded in California.\"\n",
        "predicted_relation = infer(test_sentence)\n",
        "print(f\"Predicted Relation: {predicted_relation}, {predicted_relation} = {wiki_pid.get(predicted_relation, 'Unknown')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht1EGGF3XGUV",
        "outputId": "de3837b3-3526-49f1-dc6d-b323afe78c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Relation: P159, P159 = ['headquarters location', 'specific location where an organization\\'s headquarters is or has been situated. Inverse property of \"occupant\" (P466).']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}