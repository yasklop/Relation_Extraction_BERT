{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlPg1zK_mFAt","executionInfo":{"status":"ok","timestamp":1740499388794,"user_tz":0,"elapsed":1883,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}},"outputId":"11a5f3f0-c99b-4214-be5f-5c737b4d083f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install torch transformers scikit-learn"],"metadata":{"id":"qQZKeB5HEBuW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":17520,"status":"ok","timestamp":1740499406313,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"},"user_tz":0},"id":"ZW7RcE2tS6pY"},"outputs":[],"source":["import json\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","def load_data(filename):\n","    with open(filename, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","    return data\n","\n","def preprocess_data(data):\n","    \"\"\"\n","    Preprocess the relation extraction dataset by formatting entity mentions\n","    with special markers [E1], [/E1], [E2], [/E2] for BERT-based models.\n","\n","    Args:\n","        data (dict): A dictionary where keys are relation labels and values\n","                     are lists of sentence instances containing tokenized text\n","                     and entity positions.\n","\n","    Returns:\n","        list: A list of tuples where each tuple contains a processed sentence\n","              and its corresponding relation label.\n","    \"\"\"\n","    samples = []\n","    for relation, instances in data.items():\n","        for instance in instances:\n","            # Extract \"tokens\" (words in the sentence) and entity information\n","            tokens = instance[\"tokens\"]\n","            h_text, h_id, h_pos = instance[\"h\"] # Head entity details\n","            t_text, t_id, t_pos = instance[\"t\"] # Tail entity details\n","\n","            # Add special markers around head (h) and tail (t) entities\n","            tokens[h_pos[0][0]] = \"[E1] \" + tokens[h_pos[0][0]]\n","            tokens[h_pos[-1][-1]] = tokens[h_pos[-1][-1]] + \" [/E1]\"\n","            tokens[t_pos[0][0]] = \"[E2] \" + tokens[t_pos[0][0]]\n","            tokens[t_pos[-1][-1]] = tokens[t_pos[-1][-1]] + \" [/E2]\"\n","\n","            sentence = \" \".join(tokens)\n","            samples.append((sentence, relation))\n","\n","    return samples\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"p2Q4-u8_W7Zs","executionInfo":{"status":"ok","timestamp":1740499406324,"user_tz":0,"elapsed":22,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"outputs":[],"source":["class REDataset(Dataset):\n","\n","    def __init__(self, data, tokenizer, label_encoder, max_len=128):\n","        \"\"\"\n","        Custom PyTorch dataset for relation extraction.\n","\n","        Args:\n","            data (list): List of (sentence, relation) tuples.\n","            tokenizer (BertTokenizer): Tokenizer for encoding sentences.\n","            label_encoder (LabelEncoder): Encoder for converting relation labels to numerical format.\n","            max_len (int, optional): Maximum sequence length for tokenization. Default is 128.\n","        \"\"\"\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.label_encoder = label_encoder\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a single sample from the dataset.\n","\n","        Args:\n","            idx (int): Index of the sample.\n","\n","        Returns:\n","            tuple: (input_ids, attention_mask, label_id) as PyTorch tensors.\n","        \"\"\"\n","        sentence, label = self.data[idx]\n","\n","        # Tokenize and encode the sentence\n","        encoding = self.tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n","\n","        # Extract input_ids and attention_mask, squeezing to remove extra dimensions\n","        input_ids = encoding[\"input_ids\"].squeeze(0)\n","        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n","        label_id = self.label_encoder.transform([label])[0] # Encode the relation label into a numerical ID\n","\n","        return input_ids, attention_mask, torch.tensor(label_id, dtype=torch.long)\n","\n","# BERT relation extraction model\n","class BERTRE(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BERTRE, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\") # Load pre-trained BERT model\n","        self.dropout = nn.Dropout(0.3) # Dropout layer to prevent overfitting\n","        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels) # Fully connected layer to classify relations\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) # Pass input through BERT\n","        cls_output = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token representation\n","        cls_output = self.dropout(cls_output) # Apply dropout for regularization\n","        logits = self.classifier(cls_output) # Pass through classifier to get logits\n","        return logits"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kAVhNbOwYr3b","executionInfo":{"status":"ok","timestamp":1740499406340,"user_tz":0,"elapsed":15,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"outputs":[],"source":["def train(model, dataloader, optimizer, loss_fn, device):\n","    \"\"\"\n","    Trains the model for one epoch.\n","\n","    Args:\n","        model: The neural network model.\n","        dataloader: DataLoader for the training dataset.\n","        optimizer: Optimizer for updating model weights.\n","        loss_fn: Loss function used for training.\n","        device: The device (CPU/GPU) where computations will run.\n","\n","    Returns:\n","        Tuple of (average loss, average accuracy) for the epoch.\n","    \"\"\"\n","    model.train()\n","    total_loss, total_acc = 0, 0  # Initialize total loss and accuracy accumulators\n","\n","    # Iterate over batches in the dataloader\n","    for input_ids, attention_mask, labels in dataloader:\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad() # Clear previous gradients\n","        logits = model(input_ids, attention_mask) # Forward pass: get model predictions\n","        loss = loss_fn(logits, labels)\n","        loss.backward() # Backpropagation: compute gradients\n","        optimizer.step()  # Update model parameters\n","\n","        preds = torch.argmax(logits, dim=1)  # Get predicted labels (highest logit)\n","        acc = accuracy_score(labels.cpu(), preds.cpu())\n","\n","        total_loss += loss.item()\n","        total_acc += acc\n","\n","    # Compute average loss and accuracy over all batches\n","    return total_loss / len(dataloader), total_acc / len(dataloader)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FfvIIKzjZnFm","executionInfo":{"status":"ok","timestamp":1740499406357,"user_tz":0,"elapsed":16,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"outputs":[],"source":["def evaluate(model, dataloader, device):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad(): # Disable gradient computation to save memory\n","        for input_ids, attention_mask, labels in dataloader:\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            # Forward pass: Get model predictions\n","            logits = model(input_ids, attention_mask)\n","            preds = torch.argmax(logits, dim=1) # Get the class with the highest probability\n","\n","            # Store predictions and true labels for evaluation\n","            all_preds.extend(preds.cpu().tolist())\n","            all_labels.extend(labels.cpu().tolist())\n","\n","    # Compute accuracy and generate a classification report\n","    return accuracy_score(all_labels, all_preds), classification_report(all_labels, all_preds)"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load and preprocess training data\n","train_data = load_data(\"/content/drive/MyDrive/textmining/train_wiki.json\")\n","samples = preprocess_data(train_data)\n","\n","# Encode relation labels\n","relations = list(set([r for _, r in samples]))\n","label_encoder = LabelEncoder()\n","label_encoder.fit(relations)\n","\n","# Load BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PheRqQKC1aKb","executionInfo":{"status":"ok","timestamp":1740499410720,"user_tz":0,"elapsed":4361,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}},"outputId":"6f7c4f9b-1381-4910-ec1f-c744e9cb4c4d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["414c97dc70d04d5e9bd91d47705db387","96d3caf7ba7b47f8a5a43673f0e3a6d1","9763e76d3f52495cb18feaa0264d4ad1","96e2c8ebbfd04f84a414db43cca9c914","ab4d4192b11e4f529515323e03fd985d","0558748688b74d9e93c8f1ced0ff3049","c016710b1983437f876493e6108e94ca","3d909d1f7dc8421685e427625731b2c5","99a8bd9e04e54dacbb33ed8eecccd5e2","fd512f91478e4eb881709c37b469541f","ef19256bb0dc4af7a80a7514d9aed734"]},"id":"LHhHSh0AaxgX","outputId":"a0be1b80-ec11-49a2-fb42-3db14f39f9d2","executionInfo":{"status":"ok","timestamp":1740501886675,"user_tz":0,"elapsed":2475932,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414c97dc70d04d5e9bd91d47705db387"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Loss=1.0234, Acc=0.7475\n","Epoch 2: Loss=0.3308, Acc=0.9064\n","Epoch 3: Loss=0.2064, Acc=0.9415\n","Test Accuracy: 0.8905\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.83      0.88       168\n","           1       0.90      0.85      0.88       151\n","           2       0.99      0.99      0.99       141\n","           3       1.00      1.00      1.00       138\n","           4       0.94      0.89      0.91       135\n","           5       0.98      0.99      0.98       128\n","           6       0.76      0.82      0.79       154\n","           7       0.65      0.56      0.60       144\n","           8       0.98      1.00      0.99       142\n","           9       0.79      0.82      0.80       130\n","          10       0.99      0.95      0.97       129\n","          11       0.80      0.86      0.83       142\n","          12       0.98      0.95      0.96       144\n","          13       0.95      0.93      0.94       149\n","          14       0.84      0.74      0.78       167\n","          15       0.97      0.99      0.98       154\n","          16       0.94      0.99      0.96       136\n","          17       1.00      1.00      1.00       143\n","          18       1.00      1.00      1.00       129\n","          19       0.91      0.98      0.94       125\n","          20       0.90      0.85      0.87       143\n","          21       0.66      0.83      0.74       128\n","          22       0.66      0.96      0.78       152\n","          23       0.86      0.93      0.89       135\n","          24       0.78      0.95      0.86       131\n","          25       0.80      0.73      0.76       140\n","          26       0.75      0.90      0.82       136\n","          27       0.89      0.89      0.89       142\n","          28       0.94      0.96      0.95       135\n","          29       0.98      0.96      0.97       133\n","          30       0.97      0.94      0.95       139\n","          31       0.89      0.91      0.90       146\n","          32       0.92      0.69      0.79       166\n","          33       0.96      0.90      0.93       136\n","          34       0.87      0.89      0.88       123\n","          35       0.94      0.97      0.96       152\n","          36       0.96      1.00      0.98       142\n","          37       0.78      0.92      0.84       133\n","          38       0.97      0.99      0.98       134\n","          39       0.95      0.90      0.93       133\n","          40       0.85      0.95      0.89       138\n","          41       0.98      0.79      0.88       124\n","          42       0.95      0.97      0.96       146\n","          43       0.95      0.99      0.97       146\n","          44       0.93      0.81      0.86       134\n","          45       0.89      0.96      0.92       144\n","          46       0.87      0.82      0.84       133\n","          47       0.82      0.77      0.79       141\n","          48       0.76      0.64      0.70       137\n","          49       0.83      0.94      0.88       141\n","          50       0.90      0.60      0.72       148\n","          51       0.97      0.98      0.98       129\n","          52       0.91      0.95      0.93       146\n","          53       0.88      0.82      0.85       150\n","          54       0.91      0.83      0.87       157\n","          55       0.87      0.73      0.80       128\n","          56       0.93      0.92      0.93       121\n","          57       0.92      0.94      0.93       141\n","          58       0.95      0.96      0.95       139\n","          59       0.92      0.89      0.90       141\n","          60       0.96      0.91      0.93       141\n","          61       0.73      0.87      0.79       141\n","          62       0.95      0.91      0.93       133\n","          63       0.93      0.99      0.96       133\n","\n","    accuracy                           0.89      8960\n","   macro avg       0.90      0.89      0.89      8960\n","weighted avg       0.89      0.89      0.89      8960\n","\n"]}],"source":["\n","# Create dataset\n","dataset = REDataset(samples, tokenizer, label_encoder)\n","\n","# Split dataset into 80% training and 20% validation\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","# Initialize BERT-based relation extraction model\n","model = BERTRE(num_labels=len(relations)).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Training loop\n","for epoch in range(3):\n","    loss, acc = train(model, train_loader, optimizer, loss_fn, device)\n","    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n","\n","# Evaluation on validation set\n","acc, report = evaluate(model, val_loader, device)\n","print(f\"Test Accuracy: {acc:.4f}\")\n","print(report)"]},{"cell_type":"code","source":["# torch.save(model, \"/content/drive/MyDrive/textmining/relation_extraction_model.pth\")"],"metadata":{"id":"hKv3AnKmyNwt","executionInfo":{"status":"ok","timestamp":1740501886681,"user_tz":0,"elapsed":9,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# model=torch.load('/content/drive/MyDrive/textmining/relation_extraction_model.pth')"],"metadata":{"id":"0B6SteYZjt6I","executionInfo":{"status":"ok","timestamp":1740501886685,"user_tz":0,"elapsed":2,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def infer(model, tokenizer, label_encoder, sentence, h_pos, t_pos, device):\n","    \"\"\"\n","    Perform real-time inference to predict relations between two entities in a given sentence.\n","\n","    Args:\n","        model: Trained BERT-based relation extraction model.\n","        tokenizer: BERT tokenizer for encoding sentences.\n","        label_encoder: Label encoder used for decoding predicted relation indices.\n","        sentence (str): The input sentence containing two entities.\n","        h_pos (tuple): The start and end position of the head entity in the tokenized sentence.\n","        t_pos (tuple): The start and end position of the tail entity in the tokenized sentence.\n","        device: The device (CPU/GPU) to run the model on.\n","\n","    Returns:\n","        str: Predicted relation label.\n","    \"\"\"\n","    model.eval()\n","    tokens = sentence.split()\n","\n","    # Add entity markers\n","    tokens[h_pos[0]] = \"[E1] \" + tokens[h_pos[0]]\n","    tokens[h_pos[1]] = tokens[h_pos[1]] + \" [/E1]\"\n","    tokens[t_pos[0]] = \"[E2] \" + tokens[t_pos[0]]\n","    tokens[t_pos[1]] = tokens[t_pos[1]] + \" [/E2]\"\n","    processed_sentence = \" \".join(tokens)\n","\n","    # Tokenize and encode sentence\n","    encoding = tokenizer(processed_sentence, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n","    input_ids = encoding[\"input_ids\"].to(device)\n","    attention_mask = encoding[\"attention_mask\"].to(device)\n","\n","    with torch.no_grad():\n","        logits = model(input_ids, attention_mask)\n","        pred_label_id = torch.argmax(logits, dim=1).cpu().item()\n","\n","    predicted_relation = label_encoder.inverse_transform([pred_label_id])[0]\n","    return predicted_relation\n"],"metadata":{"id":"nCv6P3qViWcP","executionInfo":{"status":"ok","timestamp":1740501886689,"user_tz":0,"elapsed":2,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Wikidata PID, name and description for each relation.\n","pid2name = load_data(\"/content/drive/MyDrive/textmining/pid2name.json\")\n","\n","# Example usage\n","sentence = \"Barack Obama was born in Honolulu, Hawaii.\"\n","h_pos = (0, 1)  # \"Barack Obama\"\n","t_pos = (5, 5)  # \"Honolulu\"\n","\n","predicted_relation = infer(model, tokenizer, label_encoder, sentence, h_pos, t_pos, device)\n","print(f\"Predicted Relation: {pid2name[predicted_relation]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKUhA_xjic5N","executionInfo":{"status":"ok","timestamp":1740501886699,"user_tz":0,"elapsed":8,"user":{"displayName":"黃芃諺","userId":"12113777317960699812"}},"outputId":"b5b75135-62c6-4b0f-a35d-5532388b42b9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Relation: ['residence', 'the place where the person is or has been, resident']\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1DtPc6dvnywRqZEaG03FSjmu2jfnuxcN3","timestamp":1740410788250}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"414c97dc70d04d5e9bd91d47705db387":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96d3caf7ba7b47f8a5a43673f0e3a6d1","IPY_MODEL_9763e76d3f52495cb18feaa0264d4ad1","IPY_MODEL_96e2c8ebbfd04f84a414db43cca9c914"],"layout":"IPY_MODEL_ab4d4192b11e4f529515323e03fd985d"}},"96d3caf7ba7b47f8a5a43673f0e3a6d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0558748688b74d9e93c8f1ced0ff3049","placeholder":"​","style":"IPY_MODEL_c016710b1983437f876493e6108e94ca","value":"model.safetensors: 100%"}},"9763e76d3f52495cb18feaa0264d4ad1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d909d1f7dc8421685e427625731b2c5","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99a8bd9e04e54dacbb33ed8eecccd5e2","value":440449768}},"96e2c8ebbfd04f84a414db43cca9c914":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd512f91478e4eb881709c37b469541f","placeholder":"​","style":"IPY_MODEL_ef19256bb0dc4af7a80a7514d9aed734","value":" 440M/440M [00:04&lt;00:00, 129MB/s]"}},"ab4d4192b11e4f529515323e03fd985d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0558748688b74d9e93c8f1ced0ff3049":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c016710b1983437f876493e6108e94ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d909d1f7dc8421685e427625731b2c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99a8bd9e04e54dacbb33ed8eecccd5e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd512f91478e4eb881709c37b469541f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef19256bb0dc4af7a80a7514d9aed734":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}